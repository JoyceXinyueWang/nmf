\section{Related work}
Researchers proposed many \textsc{nmf} algorithms. \citet{lee2001algorithms} first proposes to algorithms which minimises Euclidean distance or Kullback-Leibler divergence (\textsc{KLD}) between the original matrix and its approximation. Although this algorithm is easy to implement and have reasonable convergent rate \citep{lee2001algorithms}, it may fail on seriously corrupted dataset which violates its assumption of Gaussian noise or Poisson noise, respectively \citep{guan2017truncated}. Moreover, \citet{yang2011kullback} indicate that \textsc{KLD} is sensitive to the inital value of matrix factors and requries many iterations to retrieve from wrong initial values.  To improve the robustness of \textsc{nmf}, many methods have been proposed. \citet{lam2008non} proposes ${L_1}$-norm based \textsc{nmf} to model noisy data by a Laplace distribution which is less sensitive to outliers. However, as $L_1$-norm is not differentiable at zero, the optimization procedure is computationally expensive. \citet{kong2011robust} proposed an \textsc{nmf} algorithm using $L_{21}$-norm loss function which is robust to outliers. The updating rules used in $L_{21}$-norm \textsc{nmf}, however, converge slowly because of a continual use of the power method \citep{guan2017truncated}.

Apart from different loss functions, several optimization methods were proposed to improve the performance of \textsc{nmf}. After \citet{lee2001algorithms} proposed  multiplicative update rules \textsc{mur}, \citet{ lin2007convergence} proposed a modified \textsc{mur} which guaranteed the convergence to a stationary point. This modified \textsc{mur}, however, did not improve the convergence rate of traditional \textsc{mur} \citep{guan2012nenmf}. Moreover, as \textsc{mur} is not able to shrink all entries in matrix factors to zero, \citet{berry2007algorithms} proposed a projected nonnegative least square (\textsc{pnls}) method to overcome this problem. Although, in each nonnegative least square (\textsc{nls}) subproblem, the least squares solution is directly projected to the negative quadratic, \textsc{pnls} does not guarantee convergence \citep{guan2012nenmf}. In contrast to these gradient-based optimization methods, \citet{kim2008nonnegative} presented an active set method \textsc{as} which divides variables into two sets, free set and active set and update free set in each iteration by solving unconstrained equation. Even though \textsc{as} has good convergence rate, it assumes strictly convex in each \textsc{nls} subproblem \citep{kim2008nonnegative}.
