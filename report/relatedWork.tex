\section{Related work}
Researchers proposed many \textsc{nmf} algorithms. \citet{lee2001algorithms} first proposes ``multiplicative update rules'' to minimise Euclidean distance or Kullback-Leibler divergence between the original matrix and its approximation. Although this algorithm is easy to implement and have reasonable convergent rate \citep{lee2001algorithms}, it may fail on seriously corrupted dataset which violates its assumption of Gaussian noise or Poisson noise, respectively \citep{guan2017truncated}.  To improve the robustness of \textsc{nmf}, many methods have been proposed. \citet{lam2008non} proposes ${L_1}$-norm based \textsc{nmf} to model noisy data by a Laplace distribution which is less sensitive to outliers. However, as $L_1$-norm is not differentiable at zero, the optimization procedure is computationally expensive. \citet{kong2011robust} proposed an \textsc{nmf} algorithm using $L_{21}$-norm loss function which is robust to outliers. The updating rules used in $L_{21}$-norm \textsc{nmf}, however, converge slowly because of a continual use of the power method \citep{guan2017truncated}.
